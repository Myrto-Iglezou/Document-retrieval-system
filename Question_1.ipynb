{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "Question_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOoUR08ESUJj"
      },
      "source": [
        "## YÎ£19 Artificial Intelligence II\n",
        "# Homework 4\n",
        "\n",
        "### Iglezou Myrto - 111520170038"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klCjhjZvlqFr"
      },
      "source": [
        "\r\n",
        "<img src=\"https://venturebeat.com/wp-content/uploads/2020/03/CORD-19.png?w=1200&strip=all\" alt=\"Cord-19\" width=\"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ef8dyqplmv_"
      },
      "source": [
        "# Project Description\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjtCURBonpls"
      },
      "source": [
        "The objective of this project is about developing a document retrieval system to return titles of scientific papers containing the answer to a given user question. The dataset used in this exercise is from [COVID-19 Open Research Dataset (CORD-19)](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html), the first version. We are gonna implement 2 different sentence embedding approaches, in order for the model to retrieve the titles of the papers related to a given question.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX2R8NAZcZpK"
      },
      "source": [
        "# **Question 1** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxvA_efKqaRi"
      },
      "source": [
        "## Step 1 - Preprocess the provided dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqRYqh5T6UuN"
      },
      "source": [
        "### Read all json files from folder and keep for dataset the title and the body. Then save the dataframe to a csv file for faster reading of the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH485gzVbw88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28711858-a7ce-4f15-94e8-359a460429a7"
      },
      "source": [
        "import io\r\n",
        "import os\r\n",
        "from google.colab import drive\r\n",
        "import pandas as pd \r\n",
        "import json\r\n",
        "\r\n",
        "drive.mount('/content/drive',force_remount=True)\r\n",
        "path = r\"/content/drive/My Drive/cord-19_2020-03-13/cord-19_2020-03-13/2020-03-13/comm_use_subset\"\r\n",
        "\r\n",
        "dataset_df = pd.DataFrame(columns=['id', 'title', 'body'])\r\n",
        "\r\n",
        "for filename in os.listdir(path):\r\n",
        "   with open(os.path.join(path, filename), 'r') as f:  \r\n",
        "      json_text = json.load(f)\r\n",
        "\r\n",
        "      id = json_text['paper_id']\r\n",
        "      # print(id)\r\n",
        "      title = json_text['metadata']['title']\r\n",
        "      # print(title)\r\n",
        "      body = json_text['body_text']\r\n",
        "      # print(body)\r\n",
        "\r\n",
        "      dataset_df.loc[len(dataset_df)] = [id,title,body]\r\n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB-PP7_mspCr"
      },
      "source": [
        "dataset_df.to_csv('dataset.csv',index=False)\r\n",
        "!cp dataset.csv \"drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVGFfhAE6rAc"
      },
      "source": [
        "### Read the dataset from the csv file and save the information in a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtqng21fu4lh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de7a3c51-9b68-4d80-e974-dd7bad19b4d8"
      },
      "source": [
        "import io\r\n",
        "from google.colab import drive\r\n",
        "import pandas as pd \r\n",
        "import sys \r\n",
        "\r\n",
        "drive.mount('/content/drive',force_remount=True)\r\n",
        "filePath = r\"/content/drive/My Drive/dataset.csv\"\r\n",
        "dataset_df = pd.read_csv(filePath)\r\n",
        "dataset_df.title = dataset_df.title.astype(str)  # make everything str, for lower() function"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGlwAO3Y69yS"
      },
      "source": [
        "**Dataframe before the preprocess**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtM8rxv7WbJu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "12243451-a201-480d-86f2-611dc317c462"
      },
      "source": [
        "dataset_df.head(5)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>236bd666a76213bc131969e1d5b66e410fc1cd45</td>\n",
              "      <td>MINI REVIEW Acute Phase Proteins in Marine Mam...</td>\n",
              "      <td>[{'text': 'The mammalian immune system include...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>14374db205f6934d9cba148624462000bc6ec7be</td>\n",
              "      <td>Antibody Treatment against Angiopoietin-Like 4...</td>\n",
              "      <td>[{'text': 'IMPORTANCE Despite extensive global...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>af678e8cd31d74cdb2d690addc19d59dca331f2b</td>\n",
              "      <td>Quantifying the seasonal drivers of transmissi...</td>\n",
              "      <td>[{'text': \"Growing human population, urbanizat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>42b049c2b5b32c094dc8b10f967e43ac2169b890</td>\n",
              "      <td>Evaluation of the influenza-like illness surve...</td>\n",
              "      <td>[{'text': 'the first evaluation of the Tunisia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1664a9df618ca74e099245a2bd65f3172aeac284</td>\n",
              "      <td>nan</td>\n",
              "      <td>[{'text': 'Worldwide, lung cancer remains the ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         id  ...                                               body\n",
              "0  236bd666a76213bc131969e1d5b66e410fc1cd45  ...  [{'text': 'The mammalian immune system include...\n",
              "1  14374db205f6934d9cba148624462000bc6ec7be  ...  [{'text': 'IMPORTANCE Despite extensive global...\n",
              "2  af678e8cd31d74cdb2d690addc19d59dca331f2b  ...  [{'text': \"Growing human population, urbanizat...\n",
              "3  42b049c2b5b32c094dc8b10f967e43ac2169b890  ...  [{'text': 'the first evaluation of the Tunisia...\n",
              "4  1664a9df618ca74e099245a2bd65f3172aeac284  ...  [{'text': 'Worldwide, lung cancer remains the ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ace6Uewe7HYX"
      },
      "source": [
        "Remove some of the special characters, such as [, { , ' , : and some words form json like 'text'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YQZXAwZ36Q7"
      },
      "source": [
        "def removeCharacters(x):\r\n",
        "  x = x.str.replace(r'\\'text\\'', '')\r\n",
        "  x = x.str.replace(r'\\'start\\'', '')\r\n",
        "  x = x.str.replace(r'\\'end\\'', '')\r\n",
        "  x = x.str.replace(r'[{}]', '')\r\n",
        "  x = x.str.replace(r'[\\[\\]]', '')\r\n",
        "  x = x.str.replace(r'[\"]', '')\r\n",
        "  x = x.str.replace(r'[\\']', '')\r\n",
        "  x = x.str.replace(r'[:]', '')\r\n",
        "  x = x.str.replace(r'[()]', '')\r\n",
        "  return x"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSEosfCi3Eyk"
      },
      "source": [
        "dataset_df['body'] = removeCharacters(dataset_df['body'])"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFXBu3TJ7oVv"
      },
      "source": [
        "Remove all the uppercase letters from title and body"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLj8oa3G0WOO"
      },
      "source": [
        "dataset_df['body'] = dataset_df['body'].apply(lambda x: x.lower())\r\n",
        "# dataset_df['title'] = dataset_df['title'].apply(lambda x: x.lower())"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVNhJaYx72ie"
      },
      "source": [
        "**Dataframe after the preprocess**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPqJJQft1xP_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "3706ca7c-1551-4a1e-c33d-58251b5a2ceb"
      },
      "source": [
        "dataset_df.head(5)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>236bd666a76213bc131969e1d5b66e410fc1cd45</td>\n",
              "      <td>MINI REVIEW Acute Phase Proteins in Marine Mam...</td>\n",
              "      <td>the mammalian immune system includes innate o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>14374db205f6934d9cba148624462000bc6ec7be</td>\n",
              "      <td>Antibody Treatment against Angiopoietin-Like 4...</td>\n",
              "      <td>importance despite extensive global efforts, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>af678e8cd31d74cdb2d690addc19d59dca331f2b</td>\n",
              "      <td>Quantifying the seasonal drivers of transmissi...</td>\n",
              "      <td>growing human population, urbanization and gl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>42b049c2b5b32c094dc8b10f967e43ac2169b890</td>\n",
              "      <td>Evaluation of the influenza-like illness surve...</td>\n",
              "      <td>the first evaluation of the tunisian influenz...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1664a9df618ca74e099245a2bd65f3172aeac284</td>\n",
              "      <td>nan</td>\n",
              "      <td>worldwide, lung cancer remains the most frequ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         id  ...                                               body\n",
              "0  236bd666a76213bc131969e1d5b66e410fc1cd45  ...   the mammalian immune system includes innate o...\n",
              "1  14374db205f6934d9cba148624462000bc6ec7be  ...   importance despite extensive global efforts, ...\n",
              "2  af678e8cd31d74cdb2d690addc19d59dca331f2b  ...   growing human population, urbanization and gl...\n",
              "3  42b049c2b5b32c094dc8b10f967e43ac2169b890  ...   the first evaluation of the tunisian influenz...\n",
              "4  1664a9df618ca74e099245a2bd65f3172aeac284  ...   worldwide, lung cancer remains the most frequ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez34v_Bockvu"
      },
      "source": [
        "%%capture\r\n",
        "!pip install transformers"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii7T7CTmZG_z"
      },
      "source": [
        "%%capture\r\n",
        "!pip install sentence-transformers"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIt22mNknCAN",
        "outputId": "a4ee8c28-20a5-4581-b0d8-82935719dd1e"
      },
      "source": [
        "import torch\r\n",
        "# First checking if GPU is available\r\n",
        "train_on_gpu=torch.cuda.is_available()\r\n",
        "\r\n",
        "if(train_on_gpu):\r\n",
        "    print('Training on GPU.')\r\n",
        "    device = 'cuda'\r\n",
        "else:\r\n",
        "    print('No GPU available, training on CPU.')\r\n",
        "    device = 'cpu'"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No GPU available, training on CPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SkWJaK676jq"
      },
      "source": [
        "## Questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QSm-Z7E7535"
      },
      "source": [
        "questions =  [\r\n",
        "                  \"What are the coronoviruses?\",\r\n",
        "                  \"What was discovered in Wuhuan in December 2019?\",\r\n",
        "                  \"What is Coronovirus Disease 2019?\",\r\n",
        "                  \"What is COVID-19?\",\r\n",
        "                  \"What is caused by SARS-COV2?\",\r\n",
        "                  \"How is COVID-19 spread?\",\r\n",
        "                  \"Where was COVID-19 discovered?\",\r\n",
        "                  \"How does coronavirus spread?\",\r\n",
        "              ]"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCnCjG_4FH6w"
      },
      "source": [
        "## Create the list of sentences\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCCiwa-vFWbI",
        "outputId": "8bf834af-2e75-4b4c-ad4f-71d70abb94d1"
      },
      "source": [
        "import nltk\r\n",
        "from nltk import tokenize\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "ListOfBodies = dataset_df['body'].apply(lambda x: tokenize.sent_tokenize(x))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSqzKlSxFITl"
      },
      "source": [
        "ListOfSentences = []\r\n",
        "numOfArticles = 0\r\n",
        "for text in ListOfBodies:\r\n",
        "  ListOfSentences += text\r\n",
        "  numOfArticles+=1\r\n",
        "  if(numOfArticles == 10):\r\n",
        "    break"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUmlz-0HFlrQ"
      },
      "source": [
        "## Usefull functions for the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6IYr56uFl7e"
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "\r\n",
        "def most_similar(sentences, embeddings, query_embedding, k = 1):\r\n",
        "    X = np.stack(embeddings)\r\n",
        "    score_map = zip(sentences, cosine_similarity(X, query_embedding.reshape(1, -1)))\r\n",
        "    return sorted(score_map, key=lambda v: v[1], reverse=True)[:k]"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GylXNd8qpxt"
      },
      "source": [
        "## Step 2.a - First sentence embedding approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X67jFz8cIgOK"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvT01qEtIs9k"
      },
      "source": [
        "Uses Sentence-BERT (SBERT), a modification of the BERT network using siamese and triplet networks that is able to derive semantically meaningful sentence embeddings. This allows more efficient semantic search, which is utilized in the following application.\r\n",
        "\r\n",
        "The siamese network architecture enables that fixed-sized vectors for input sentences can be derived. Using a similarity measure like cosine similarity or Manhatten / Euclidean distance, semantically similar sentences can be found. Cosine similarity is used in this work.\r\n",
        "\r\n",
        "SBERT fine tuned on NLI data which creates SOTA sentence embeddings, as reported in the [SBERT paper](https://arxiv.org/pdf/1908.10084.pdf).\r\n",
        "\r\n",
        "SBERT Framework example\r\n",
        "\r\n",
        "<img src=\"https://combine.se/wp-content/uploads/2019/09/3.png\" alt=\"Cord-19\" width=\"300\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkditrFyaKhe"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "from sentence_transformers import SentenceTransformer\r\n",
        "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens',device=device)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY6BmUj--Aav"
      },
      "source": [
        "def ask_question(question,ListOfSentences,sentence_embeddings,dataset_df):\r\n",
        "  query_vec = sbert_model.encode([question])[0]\r\n",
        "  similar = most_similar(ListOfSentences,sentence_embeddings,query_vec)\r\n",
        "  row = dataset_df[dataset_df['body'].str.contains(similar[0][0])]\r\n",
        "  title = row['title'].tolist()[0]\r\n",
        "  text = similar[0][0]\r\n",
        "\r\n",
        "  return title,text"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNy5I8Hp-CLD"
      },
      "source": [
        "from termcolor import colored\r\n",
        "\r\n",
        "def print_answer(question,title,text):\r\n",
        "\r\n",
        "  print(colored(\"Question : \",attrs=['bold']),question)\r\n",
        "  print(\"\\n\")\r\n",
        "  print(colored(\"Title : \",attrs=['bold']),title)\r\n",
        "  print(\"\\n\")\r\n",
        "  print(colored(\"Text : \",attrs=['bold']),text)\r\n",
        "  print(\"\\n\")\r\n",
        "  # text = row['body'].apply(lambda x: tokenize.sent_tokenize(x))\r\n",
        "  # num = 0\r\n",
        "  # for t in text:\r\n",
        "  #   for sentence in t:\r\n",
        "  #     if sentence == l1:\r\n",
        "  #         print(t[num-1], end=\" \")\r\n",
        "  #         print(\"\\n\")\r\n",
        "  #         print(colored(sentence,'grey','on_yellow'), end=\" \")\r\n",
        "  #         print(\"\\n\")\r\n",
        "  #         print(t[num+1], end=\" \")\r\n",
        "  #         num+=1\r\n",
        "  # print(\"\\n\")"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BlZiiy9VHsx"
      },
      "source": [
        "sentence_embeddings = sbert_model.encode(ListOfSentences,device=device)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nTaYmUq9HrT",
        "outputId": "41d5bfeb-4c6c-43b2-c944-5f80d4c7f8f5"
      },
      "source": [
        "for question in questions: \r\n",
        "  title,text = ask_question(question,ListOfSentences,sentence_embeddings,dataset_df)\r\n",
        "  print(\"-------------------------------------------------------------------------\")\r\n",
        "  print_answer(question, title, text)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------------------------\n",
            "\u001b[1mQuestion : \u001b[0m What are the coronoviruses?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m Antibody Treatment against Angiopoietin-Like 4 Reduces Pulmonary Edema and Injury in Secondary Pneumococcal Pneumonia\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m 3a and b and immunoneutralization of cangptl4  fig.\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\u001b[1mQuestion : \u001b[0m What was discovered in Wuhuan in December 2019?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m ADAM17-dependent signaling is required for oncogenic human papillomavirus entry platform assembly\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m what triggers the assembly of the secondary entry receptor complex, that contains cd151 and egfr as functional components mikulicËicÂ´and florin, 2019 .\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\u001b[1mQuestion : \u001b[0m What is Coronovirus Disease 2019?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m ADAM17-dependent signaling is required for oncogenic human papillomavirus entry platform assembly\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m what triggers the assembly of the secondary entry receptor complex, that contains cd151 and egfr as functional components mikulicËicÂ´and florin, 2019 .\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\u001b[1mQuestion : \u001b[0m What is COVID-19?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m Quantifying the seasonal drivers of transmission for Lassa fever in Nigeria\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m 1 in 19 .\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\u001b[1mQuestion : \u001b[0m What is caused by SARS-COV2?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m Antibody Treatment against Angiopoietin-Like 4 Reduces Pulmonary Edema and Injury in Secondary Pneumococcal Pneumonia\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m 4c, ref_id figref5, section results,  immunoneutralization of pneumococcal pneumolysin abrogates its poreforming action on alveolar epithelium.\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\u001b[1mQuestion : \u001b[0m How is COVID-19 spread?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m Quantifying the seasonal drivers of transmission for Lassa fever in Nigeria\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m 1 in 19, ref_id figref0, section material and methods a data collection,  first, we accounted for two transmission routes of lf infection, i.e.\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\u001b[1mQuestion : \u001b[0m Where was COVID-19 discovered?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m ADAM17-dependent signaling is required for oncogenic human papillomavirus entry platform assembly\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m data n = 128-152 images were analyzed using wilcoxon rank sum test p=7.97e-06.\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\u001b[1mQuestion : \u001b[0m How does coronavirus spread?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m ADAM17-dependent signaling is required for oncogenic human papillomavirus entry platform assembly\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m  viral infections by human papillomaviruses hpvs cause benign warts and malignant tumors.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVrg1Ba2q5p0"
      },
      "source": [
        "## Step 2.b - Second sentence embedding approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VyWhhZAEW0j",
        "outputId": "8ffd125d-bc5f-4c33-8f63-f18073db6366"
      },
      "source": [
        "! mkdir encoder\r\n",
        "! curl -Lo encoder/infersent2.pkl https://dl.fbaipublicfiles.com/infersent/infersent2.pkl\r\n",
        "  \r\n",
        "! mkdir GloVe\r\n",
        "! curl -Lo GloVe/glove.840B.300d.zip http://nlp.stanford.edu/data/glove.840B.300d.zip\r\n",
        "! unzip GloVe/glove.840B.300d.zip -d GloVe/"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  146M  100  146M    0     0  10.2M      0  0:00:14  0:00:14 --:--:-- 11.8M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0   315    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0   352    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 2075M  100 2075M    0     0  2086k      0  0:16:58  0:16:58 --:--:-- 2360k\n",
            "Archive:  GloVe/glove.840B.300d.zip\n",
            "  inflating: GloVe/glove.840B.300d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLyEMksVr7V_",
        "outputId": "ae48c31b-7c68-4a24-f14e-e1724510af65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "drive.mount('/content/drive',force_remount=True)\r\n",
        "sys.path.append('/content/drive/My Drive/')\r\n",
        "!cp -r \"/content/drive/My Drive/models.py\" '/content/'"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WENjmLyhEa7s"
      },
      "source": [
        "import models\r\n",
        "from models import InferSent\r\n",
        "import torch \r\n",
        "\r\n",
        "V = 2\r\n",
        "MODEL_PATH = 'encoder/infersent%s.pkl' % V\r\n",
        "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\r\n",
        "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\r\n",
        "model = InferSent(params_model)\r\n",
        "model.load_state_dict(torch.load(MODEL_PATH))\r\n",
        "\r\n",
        "W2V_PATH = '/content/GloVe/glove.840B.300d.txt'\r\n",
        "model.set_w2v_path(W2V_PATH)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9mB0yD6Eeck"
      },
      "source": [
        "from models import InferSent\r\n",
        "import torch\r\n",
        "\r\n",
        "V = 2\r\n",
        "MODEL_PATH = 'encoder/infersent%s.pkl' % V\r\n",
        "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 384,\r\n",
        "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\r\n",
        "model = InferSent(params_model)\r\n",
        "model.load_state_dict(torch.load(MODEL_PATH))\r\n",
        "\r\n",
        "W2V_PATH = '/content/GloVe/glove.840B.300d.txt'\r\n",
        "model.set_w2v_path(W2V_PATH)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5puzY_qm5tGI"
      },
      "source": [
        "if(train_on_gpu):\r\n",
        "  model = model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aO9k1z5wEjn7",
        "outputId": "21267a8d-c760-463b-ad55-8ac3783086c4"
      },
      "source": [
        "model.build_vocab(ListOfSentences, tokenize=True)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6721(/8039) words with w2v vectors\n",
            "Vocab size : 6721\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4Bl7q_rsoCT",
        "outputId": "e5920fed-f4f2-4805-d5d6-2194bc3fb076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "InferSent_embeddings = []\r\n",
        "\r\n",
        "for sentence in ListOfSentences:\r\n",
        " InferSent_embeddings.append(model.encode(sentence)[0])"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models.py:207: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  sentences = np.array(sentences)[idx_sort]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-da6e4a1c3825>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mListOfSentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m  \u001b[0mInferSent_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/models.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, bsize, tokenize, verbose)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstidx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstidx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbsize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sent_tuple)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Handling padding in Recurrent Networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0msent_packed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_len_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0msent_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_packed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# seqlen x batch x 2*nhid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0msent_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0;32m--> 585\u001b[0;31m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[1;32m    586\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXd2_Nh5zrrB",
        "outputId": "e63893f5-8ed6-4a23-d032-3bddb71b7bc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(InferSent_embeddings[0])"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4096"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMQvkMZUtxcl",
        "outputId": "030f5fb8-1e4b-4807-8fd4-6306ad351b90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "for question in questions: \r\n",
        "  title,text = ask_question(question,ListOfSentences,InferSent_embeddings,dataset_df)\r\n",
        "  print(\"-------------------------------------------------------------------------\")\r\n",
        "  print_answer(question, title, text)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-107-61860964a89d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mask_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mListOfSentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mInferSent_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------------------------------------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-19790aff3f01>\u001b[0m in \u001b[0;36mask_question\u001b[0;34m(question, ListOfSentences, sentence_embeddings, dataset_df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mask_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mListOfSentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mquery_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msbert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0msimilar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mListOfSentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-b9c160133377>\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(sentences, embeddings, query_embedding, k)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mscore_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[0;31m# to avoid recursive import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    153\u001b[0m         raise ValueError(\"Incompatible dimension for X and Y matrices: \"\n\u001b[1;32m    154\u001b[0m                          \"X.shape[1] == %d while Y.shape[1] == %d\" % (\n\u001b[0;32m--> 155\u001b[0;31m                              X.shape[1], Y.shape[1]))\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Incompatible dimension for X and Y matrices: X.shape[1] == 4096 while Y.shape[1] == 768"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWSAT3IxrBZ4"
      },
      "source": [
        "## Step 3 - Comparison of two models"
      ]
    }
  ]
}