{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "Question_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOoUR08ESUJj"
      },
      "source": [
        "## YÎ£19 Artificial Intelligence II\n",
        "# Homework 4\n",
        "\n",
        "### Iglezou Myrto - 111520170038"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klCjhjZvlqFr"
      },
      "source": [
        "\r\n",
        "<img src=\"https://venturebeat.com/wp-content/uploads/2020/03/CORD-19.png?w=1200&strip=all\" alt=\"Cord-19\" width=\"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ef8dyqplmv_"
      },
      "source": [
        "# Project Description\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjtCURBonpls"
      },
      "source": [
        "The objective of this project is about developing a document retrieval system to return titles of scientific papers containing the answer to a given user question. The dataset used in this exercise is from [COVID-19 Open Research Dataset (CORD-19)](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html), the first version. We are gonna implement 2 different sentence embedding approaches, in order for the model to retrieve the titles of the papers related to a given question.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX2R8NAZcZpK"
      },
      "source": [
        "# **Question 1** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxvA_efKqaRi"
      },
      "source": [
        "## Step 1 - Preprocess the provided dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqRYqh5T6UuN"
      },
      "source": [
        "### Read all json files from folder and keep for dataset the title and the body. Then save the dataframe to a csv file for faster reading of the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH485gzVbw88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28711858-a7ce-4f15-94e8-359a460429a7"
      },
      "source": [
        "import io\r\n",
        "import os\r\n",
        "from google.colab import drive\r\n",
        "import pandas as pd \r\n",
        "import json\r\n",
        "\r\n",
        "drive.mount('/content/drive',force_remount=True)\r\n",
        "path = r\"/content/drive/My Drive/cord-19_2020-03-13/cord-19_2020-03-13/2020-03-13/comm_use_subset\"\r\n",
        "\r\n",
        "dataset_df = pd.DataFrame(columns=['id', 'title', 'body'])\r\n",
        "\r\n",
        "for filename in os.listdir(path):\r\n",
        "   with open(os.path.join(path, filename), 'r') as f:  \r\n",
        "      json_text = json.load(f)\r\n",
        "\r\n",
        "      id = json_text['paper_id']\r\n",
        "      # print(id)\r\n",
        "      title = json_text['metadata']['title']\r\n",
        "      # print(title)\r\n",
        "      body = json_text['body_text']\r\n",
        "      # print(body)\r\n",
        "\r\n",
        "      dataset_df.loc[len(dataset_df)] = [id,title,body]\r\n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB-PP7_mspCr"
      },
      "source": [
        "dataset_df.to_csv('dataset.csv',index=False)\r\n",
        "!cp dataset.csv \"drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVGFfhAE6rAc"
      },
      "source": [
        "### Read the dataset from the csv file and save the information in a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtqng21fu4lh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de7a3c51-9b68-4d80-e974-dd7bad19b4d8"
      },
      "source": [
        "import io\r\n",
        "from google.colab import drive\r\n",
        "import pandas as pd \r\n",
        "import sys \r\n",
        "\r\n",
        "drive.mount('/content/drive',force_remount=True)\r\n",
        "filePath = r\"/content/drive/My Drive/dataset.csv\"\r\n",
        "dataset_df = pd.read_csv(filePath)\r\n",
        "dataset_df.title = dataset_df.title.astype(str)  # make everything str, for lower() function"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGlwAO3Y69yS"
      },
      "source": [
        "**Dataframe before the preprocess**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtM8rxv7WbJu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "12243451-a201-480d-86f2-611dc317c462"
      },
      "source": [
        "dataset_df.head(5)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>236bd666a76213bc131969e1d5b66e410fc1cd45</td>\n",
              "      <td>MINI REVIEW Acute Phase Proteins in Marine Mam...</td>\n",
              "      <td>[{'text': 'The mammalian immune system include...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>14374db205f6934d9cba148624462000bc6ec7be</td>\n",
              "      <td>Antibody Treatment against Angiopoietin-Like 4...</td>\n",
              "      <td>[{'text': 'IMPORTANCE Despite extensive global...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>af678e8cd31d74cdb2d690addc19d59dca331f2b</td>\n",
              "      <td>Quantifying the seasonal drivers of transmissi...</td>\n",
              "      <td>[{'text': \"Growing human population, urbanizat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>42b049c2b5b32c094dc8b10f967e43ac2169b890</td>\n",
              "      <td>Evaluation of the influenza-like illness surve...</td>\n",
              "      <td>[{'text': 'the first evaluation of the Tunisia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1664a9df618ca74e099245a2bd65f3172aeac284</td>\n",
              "      <td>nan</td>\n",
              "      <td>[{'text': 'Worldwide, lung cancer remains the ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         id  ...                                               body\n",
              "0  236bd666a76213bc131969e1d5b66e410fc1cd45  ...  [{'text': 'The mammalian immune system include...\n",
              "1  14374db205f6934d9cba148624462000bc6ec7be  ...  [{'text': 'IMPORTANCE Despite extensive global...\n",
              "2  af678e8cd31d74cdb2d690addc19d59dca331f2b  ...  [{'text': \"Growing human population, urbanizat...\n",
              "3  42b049c2b5b32c094dc8b10f967e43ac2169b890  ...  [{'text': 'the first evaluation of the Tunisia...\n",
              "4  1664a9df618ca74e099245a2bd65f3172aeac284  ...  [{'text': 'Worldwide, lung cancer remains the ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ace6Uewe7HYX"
      },
      "source": [
        "Remove some of the special characters, such as [, { , ' , : and some words form json like 'text'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YQZXAwZ36Q7"
      },
      "source": [
        "def removeCharacters(x):\r\n",
        "  x = x.str.replace(r'\\'text\\'', '')\r\n",
        "  x = x.str.replace(r'\\'start\\'', '')\r\n",
        "  x = x.str.replace(r'\\'end\\'', '')\r\n",
        "  x = x.str.replace(r'[{}]', '')\r\n",
        "  x = x.str.replace(r'[\\[\\]]', '')\r\n",
        "  x = x.str.replace(r'[\"]', '')\r\n",
        "  x = x.str.replace(r'[\\']', '')\r\n",
        "  x = x.str.replace(r'[:]', '')\r\n",
        "  x = x.str.replace(r'[()]', '')\r\n",
        "  return x"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSEosfCi3Eyk"
      },
      "source": [
        "dataset_df['body'] = removeCharacters(dataset_df['body'])"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFXBu3TJ7oVv"
      },
      "source": [
        "Remove all the uppercase letters from title and body"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLj8oa3G0WOO"
      },
      "source": [
        "dataset_df['body'] = dataset_df['body'].apply(lambda x: x.lower())\r\n",
        "# dataset_df['title'] = dataset_df['title'].apply(lambda x: x.lower())"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVNhJaYx72ie"
      },
      "source": [
        "**Dataframe after the preprocess**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPqJJQft1xP_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "3706ca7c-1551-4a1e-c33d-58251b5a2ceb"
      },
      "source": [
        "dataset_df.head(5)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>236bd666a76213bc131969e1d5b66e410fc1cd45</td>\n",
              "      <td>MINI REVIEW Acute Phase Proteins in Marine Mam...</td>\n",
              "      <td>the mammalian immune system includes innate o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>14374db205f6934d9cba148624462000bc6ec7be</td>\n",
              "      <td>Antibody Treatment against Angiopoietin-Like 4...</td>\n",
              "      <td>importance despite extensive global efforts, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>af678e8cd31d74cdb2d690addc19d59dca331f2b</td>\n",
              "      <td>Quantifying the seasonal drivers of transmissi...</td>\n",
              "      <td>growing human population, urbanization and gl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>42b049c2b5b32c094dc8b10f967e43ac2169b890</td>\n",
              "      <td>Evaluation of the influenza-like illness surve...</td>\n",
              "      <td>the first evaluation of the tunisian influenz...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1664a9df618ca74e099245a2bd65f3172aeac284</td>\n",
              "      <td>nan</td>\n",
              "      <td>worldwide, lung cancer remains the most frequ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         id  ...                                               body\n",
              "0  236bd666a76213bc131969e1d5b66e410fc1cd45  ...   the mammalian immune system includes innate o...\n",
              "1  14374db205f6934d9cba148624462000bc6ec7be  ...   importance despite extensive global efforts, ...\n",
              "2  af678e8cd31d74cdb2d690addc19d59dca331f2b  ...   growing human population, urbanization and gl...\n",
              "3  42b049c2b5b32c094dc8b10f967e43ac2169b890  ...   the first evaluation of the tunisian influenz...\n",
              "4  1664a9df618ca74e099245a2bd65f3172aeac284  ...   worldwide, lung cancer remains the most frequ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez34v_Bockvu"
      },
      "source": [
        "%%capture\r\n",
        "!pip install transformers"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii7T7CTmZG_z"
      },
      "source": [
        "%%capture\r\n",
        "!pip install sentence-transformers"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIt22mNknCAN",
        "outputId": "a4ee8c28-20a5-4581-b0d8-82935719dd1e"
      },
      "source": [
        "import torch\r\n",
        "# First checking if GPU is available\r\n",
        "train_on_gpu=torch.cuda.is_available()\r\n",
        "\r\n",
        "if(train_on_gpu):\r\n",
        "    print('Training on GPU.')\r\n",
        "    device = 'cuda'\r\n",
        "else:\r\n",
        "    print('No GPU available, training on CPU.')\r\n",
        "    device = 'cpu'"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No GPU available, training on CPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SkWJaK676jq"
      },
      "source": [
        "## Questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QSm-Z7E7535"
      },
      "source": [
        "questions =  [\r\n",
        "                  \"What are the coronoviruses?\",\r\n",
        "                  \"What was discovered in Wuhuan in December 2019?\",\r\n",
        "                  \"What is Coronovirus Disease 2019?\",\r\n",
        "                  \"What is COVID-19?\",\r\n",
        "                  \"What is caused by SARS-COV2?\",\r\n",
        "                  \"How is COVID-19 spread?\",\r\n",
        "                  \"Where was COVID-19 discovered?\",\r\n",
        "                  \"How does coronavirus spread?\",\r\n",
        "              ]"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCnCjG_4FH6w"
      },
      "source": [
        "## Create the list of sentences\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCCiwa-vFWbI",
        "outputId": "8bf834af-2e75-4b4c-ad4f-71d70abb94d1"
      },
      "source": [
        "import nltk\r\n",
        "from nltk import tokenize\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "ListOfBodies = dataset_df['body'].apply(lambda x: tokenize.sent_tokenize(x))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSqzKlSxFITl"
      },
      "source": [
        "ListOfSentences = []\r\n",
        "numOfArticles = 0\r\n",
        "for text in ListOfBodies:\r\n",
        "  ListOfSentences += text\r\n",
        "  numOfArticles+=1\r\n",
        "  if(numOfArticles == 10):\r\n",
        "    break"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUmlz-0HFlrQ"
      },
      "source": [
        "## Usefull functions for the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6IYr56uFl7e"
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "\r\n",
        "def most_similar(sentences, embeddings, query_embedding, k = 1):\r\n",
        "    X = np.stack(embeddings)\r\n",
        "    score_map = zip(sentences, cosine_similarity(X, query_embedding.reshape(1, -1)))\r\n",
        "    return sorted(score_map, key=lambda v: v[1], reverse=True)[:k]"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GylXNd8qpxt"
      },
      "source": [
        "## Step 2.a - First sentence embedding approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X67jFz8cIgOK"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvT01qEtIs9k"
      },
      "source": [
        "Uses Sentence-BERT (SBERT), a modification of the BERT network using siamese and triplet networks that is able to derive semantically meaningful sentence embeddings. This allows more efficient semantic search, which is utilized in the following application.\r\n",
        "\r\n",
        "The siamese network architecture enables that fixed-sized vectors for input sentences can be derived. Using a similarity measure like cosine similarity or Manhatten / Euclidean distance, semantically similar sentences can be found. Cosine similarity is used in this work.\r\n",
        "\r\n",
        "SBERT fine tuned on NLI data which creates SOTA sentence embeddings, as reported in the [SBERT paper](https://arxiv.org/pdf/1908.10084.pdf).\r\n",
        "\r\n",
        "SBERT Framework example\r\n",
        "\r\n",
        "<img src=\"https://combine.se/wp-content/uploads/2019/09/3.png\" alt=\"Cord-19\" width=\"300\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkditrFyaKhe"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "from sentence_transformers import SentenceTransformer\r\n",
        "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens',device=device)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY6BmUj--Aav"
      },
      "source": [
        "def ask_question(question,ListOfSentences,sentence_embeddings,dataset_df):\r\n",
        "  query_vec = sbert_model.encode([question])[0]\r\n",
        "  similar = most_similar(ListOfSentences,sentence_embeddings,query_vec)\r\n",
        "  row = dataset_df[dataset_df['body'].str.contains(similar[0][0])]\r\n",
        "  title = row['title'].tolist()[0]\r\n",
        "  text = similar[0][0]\r\n",
        "\r\n",
        "  return title,text"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNy5I8Hp-CLD"
      },
      "source": [
        "from termcolor import colored\r\n",
        "\r\n",
        "def print_answer(question,title,text):\r\n",
        "\r\n",
        "  print(colored(\"Question : \",attrs=['bold']),question)\r\n",
        "  print(\"\\n\")\r\n",
        "  print(colored(\"Title : \",attrs=['bold']),title)\r\n",
        "  print(\"\\n\")\r\n",
        "  print(colored(\"Text : \",attrs=['bold']),text)\r\n",
        "  print(\"\\n\")\r\n",
        "  # text = row['body'].apply(lambda x: tokenize.sent_tokenize(x))\r\n",
        "  # num = 0\r\n",
        "  # for t in text:\r\n",
        "  #   for sentence in t:\r\n",
        "  #     if sentence == l1:\r\n",
        "  #         print(t[num-1], end=\" \")\r\n",
        "  #         print(\"\\n\")\r\n",
        "  #         print(colored(sentence,'grey','on_yellow'), end=\" \")\r\n",
        "  #         print(\"\\n\")\r\n",
        "  #         print(t[num+1], end=\" \")\r\n",
        "  #         num+=1\r\n",
        "  # print(\"\\n\")"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BlZiiy9VHsx"
      },
      "source": [
        "sentence_embeddings = sbert_model.encode(ListOfSentences,device=device)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nTaYmUq9HrT",
        "outputId": "41d5bfeb-4c6c-43b2-c944-5f80d4c7f8f5"
      },
      "source": [
        "for question in questions: \r\n",
        "  title,text = ask_question(question,ListOfSentences,sentence_embeddings,dataset_df)\r\n",
        "  print(\"-------------------------------------------------------------------------\")\r\n",
        "  print_answer(question, title, text)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------------------------\n",
            "\u001b[1mQuestion : \u001b[0m What are the coronoviruses?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m Antibody Treatment against Angiopoietin-Like 4 Reduces Pulmonary Edema and Injury in Secondary Pneumococcal Pneumonia\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m 3a and b and immunoneutralization of cangptl4  fig.\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\u001b[1mQuestion : \u001b[0m What was discovered in Wuhuan in December 2019?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m ADAM17-dependent signaling is required for oncogenic human papillomavirus entry platform assembly\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m what triggers the assembly of the secondary entry receptor complex, that contains cd151 and egfr as functional components mikulicËicÂ´and florin, 2019 .\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\u001b[1mQuestion : \u001b[0m What is Coronovirus Disease 2019?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m ADAM17-dependent signaling is required for oncogenic human papillomavirus entry platform assembly\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m what triggers the assembly of the secondary entry receptor complex, that contains cd151 and egfr as functional components mikulicËicÂ´and florin, 2019 .\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\u001b[1mQuestion : \u001b[0m What is COVID-19?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m Quantifying the seasonal drivers of transmission for Lassa fever in Nigeria\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m 1 in 19 .\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\u001b[1mQuestion : \u001b[0m What is caused by SARS-COV2?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m Antibody Treatment against Angiopoietin-Like 4 Reduces Pulmonary Edema and Injury in Secondary Pneumococcal Pneumonia\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m 4c, ref_id figref5, section results,  immunoneutralization of pneumococcal pneumolysin abrogates its poreforming action on alveolar epithelium.\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\u001b[1mQuestion : \u001b[0m How is COVID-19 spread?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m Quantifying the seasonal drivers of transmission for Lassa fever in Nigeria\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m 1 in 19, ref_id figref0, section material and methods a data collection,  first, we accounted for two transmission routes of lf infection, i.e.\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\u001b[1mQuestion : \u001b[0m Where was COVID-19 discovered?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m ADAM17-dependent signaling is required for oncogenic human papillomavirus entry platform assembly\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m data n = 128-152 images were analyzed using wilcoxon rank sum test p=7.97e-06.\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\u001b[1mQuestion : \u001b[0m How does coronavirus spread?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m ADAM17-dependent signaling is required for oncogenic human papillomavirus entry platform assembly\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m  viral infections by human papillomaviruses hpvs cause benign warts and malignant tumors.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVrg1Ba2q5p0"
      },
      "source": [
        "## Step 2.b - Second sentence embedding approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VyWhhZAEW0j",
        "outputId": "8ffd125d-bc5f-4c33-8f63-f18073db6366"
      },
      "source": [
        "! mkdir encoder\r\n",
        "! curl -Lo encoder/infersent2.pkl https://dl.fbaipublicfiles.com/infersent/infersent2.pkl\r\n",
        "  \r\n",
        "! mkdir GloVe\r\n",
        "! curl -Lo GloVe/glove.840B.300d.zip http://nlp.stanford.edu/data/glove.840B.300d.zip\r\n",
        "! unzip GloVe/glove.840B.300d.zip -d GloVe/"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  146M  100  146M    0     0  10.2M      0  0:00:14  0:00:14 --:--:-- 11.8M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0   315    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0   352    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 2075M  100 2075M    0     0  2086k      0  0:16:58  0:16:58 --:--:-- 2360k\n",
            "Archive:  GloVe/glove.840B.300d.zip\n",
            "  inflating: GloVe/glove.840B.300d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "WENjmLyhEa7s",
        "outputId": "68510b49-ef0a-424b-de5a-d48f45447fc1"
      },
      "source": [
        "import models\r\n",
        "from models import InferSent\r\n",
        "import torch \r\n",
        "\r\n",
        "V = 2\r\n",
        "MODEL_PATH = 'encoder/infersent%s.pkl' % V\r\n",
        "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\r\n",
        "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\r\n",
        "model = InferSent(params_model)\r\n",
        "model.load_state_dict(torch.load(MODEL_PATH))\r\n",
        "\r\n",
        "W2V_PATH = '/content/GloVe/glove.840B.300d.txt'\r\n",
        "model.set_w2v_path(W2V_PATH)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4087c27d-2526-4a52-89bd-c44dee894455\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4087c27d-2526-4a52-89bd-c44dee894455\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-47a7f47753f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models.py'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInferSent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m   result = _output.eval_js(\n\u001b[1;32m     63\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m---> 64\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Cannot read property '_uploadFiles' of undefined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9mB0yD6Eeck"
      },
      "source": [
        "from models import InferSent\r\n",
        "import torch\r\n",
        "\r\n",
        "V = 2\r\n",
        "MODEL_PATH = 'encoder/infersent%s.pkl' % V\r\n",
        "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\r\n",
        "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\r\n",
        "model = InferSent(params_model)\r\n",
        "model.load_state_dict(torch.load(MODEL_PATH))\r\n",
        "\r\n",
        "W2V_PATH = '/content/GloVe/glove.840B.300d.txt'\r\n",
        "model.set_w2v_path(W2V_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO9k1z5wEjn7"
      },
      "source": [
        "model.build_vocab(sentences, tokenize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWSAT3IxrBZ4"
      },
      "source": [
        "## Step 3 - Comparison of two models"
      ]
    }
  ]
}