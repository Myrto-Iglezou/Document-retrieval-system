{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "Question_1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Myrto-Iglezou/AI2-project4/blob/master/Question_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOoUR08ESUJj"
      },
      "source": [
        "## YΣ19 Artificial Intelligence II\n",
        "# Homework 4\n",
        "\n",
        "### Iglezou Myrto - 111520170038"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klCjhjZvlqFr"
      },
      "source": [
        "\r\n",
        "<img src=\"https://venturebeat.com/wp-content/uploads/2020/03/CORD-19.png?w=1200&strip=all\" alt=\"Cord-19\" width=\"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ef8dyqplmv_"
      },
      "source": [
        "# Project Description\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjtCURBonpls"
      },
      "source": [
        "The objective of this project is about developing a document retrieval system to return titles of scientific papers containing the answer to a given user question. The dataset used in this exercise is from [COVID-19 Open Research Dataset (CORD-19)](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html), the first version. We are gonna implement 2 different sentence embedding approaches, in order for the model to retrieve the titles of the papers related to a given question.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX2R8NAZcZpK"
      },
      "source": [
        "# **Question 1** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxvA_efKqaRi"
      },
      "source": [
        "## Step 1 - Preprocess the provided dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqRYqh5T6UuN"
      },
      "source": [
        "### Read all json files from folder and keep for dataset the title and the body. Then save the dataframe to a csv file for faster reading of the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH485gzVbw88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28711858-a7ce-4f15-94e8-359a460429a7"
      },
      "source": [
        "import io\r\n",
        "import os\r\n",
        "from google.colab import drive\r\n",
        "import pandas as pd \r\n",
        "import json\r\n",
        "\r\n",
        "drive.mount('/content/drive',force_remount=True)\r\n",
        "path = r\"/content/drive/My Drive/cord-19_2020-03-13/cord-19_2020-03-13/2020-03-13/comm_use_subset\"\r\n",
        "\r\n",
        "dataset_df = pd.DataFrame(columns=['id', 'title', 'body'])\r\n",
        "\r\n",
        "for filename in os.listdir(path):\r\n",
        "   with open(os.path.join(path, filename), 'r') as f:  \r\n",
        "      json_text = json.load(f)\r\n",
        "\r\n",
        "      id = json_text['paper_id']\r\n",
        "      # print(id)\r\n",
        "      title = json_text['metadata']['title']\r\n",
        "      # print(title)\r\n",
        "      body = json_text['body_text']\r\n",
        "      # print(body)\r\n",
        "\r\n",
        "      dataset_df.loc[len(dataset_df)] = [id,title,body]\r\n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB-PP7_mspCr"
      },
      "source": [
        "dataset_df.to_csv('dataset.csv',index=False)\r\n",
        "!cp dataset.csv \"drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVGFfhAE6rAc"
      },
      "source": [
        "### Read the dataset from the csv file and save the information in a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtqng21fu4lh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6079304d-5f4f-4cb4-ac84-7420f29d54bc"
      },
      "source": [
        "import io\r\n",
        "from google.colab import drive\r\n",
        "import pandas as pd \r\n",
        "import sys \r\n",
        "\r\n",
        "drive.mount('/content/drive',force_remount=True)\r\n",
        "filePath = r\"/content/drive/My Drive/dataset.csv\"\r\n",
        "dataset_df = pd.read_csv(filePath)\r\n",
        "dataset_df.title = dataset_df.title.astype(str)  # make everything str, for lower() function"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGlwAO3Y69yS"
      },
      "source": [
        "**Dataframe before the preprocess**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtM8rxv7WbJu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "6923e282-6553-4c07-8466-84dbf353c99c"
      },
      "source": [
        "dataset_df.head(5)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>236bd666a76213bc131969e1d5b66e410fc1cd45</td>\n",
              "      <td>MINI REVIEW Acute Phase Proteins in Marine Mam...</td>\n",
              "      <td>[{'text': 'The mammalian immune system include...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>14374db205f6934d9cba148624462000bc6ec7be</td>\n",
              "      <td>Antibody Treatment against Angiopoietin-Like 4...</td>\n",
              "      <td>[{'text': 'IMPORTANCE Despite extensive global...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>af678e8cd31d74cdb2d690addc19d59dca331f2b</td>\n",
              "      <td>Quantifying the seasonal drivers of transmissi...</td>\n",
              "      <td>[{'text': \"Growing human population, urbanizat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>42b049c2b5b32c094dc8b10f967e43ac2169b890</td>\n",
              "      <td>Evaluation of the influenza-like illness surve...</td>\n",
              "      <td>[{'text': 'the first evaluation of the Tunisia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1664a9df618ca74e099245a2bd65f3172aeac284</td>\n",
              "      <td>nan</td>\n",
              "      <td>[{'text': 'Worldwide, lung cancer remains the ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         id  ...                                               body\n",
              "0  236bd666a76213bc131969e1d5b66e410fc1cd45  ...  [{'text': 'The mammalian immune system include...\n",
              "1  14374db205f6934d9cba148624462000bc6ec7be  ...  [{'text': 'IMPORTANCE Despite extensive global...\n",
              "2  af678e8cd31d74cdb2d690addc19d59dca331f2b  ...  [{'text': \"Growing human population, urbanizat...\n",
              "3  42b049c2b5b32c094dc8b10f967e43ac2169b890  ...  [{'text': 'the first evaluation of the Tunisia...\n",
              "4  1664a9df618ca74e099245a2bd65f3172aeac284  ...  [{'text': 'Worldwide, lung cancer remains the ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ace6Uewe7HYX"
      },
      "source": [
        "Remove some of the special characters, such as [, { , ' , : and some words form json like 'text'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YQZXAwZ36Q7"
      },
      "source": [
        "def removeCharacters(x):\r\n",
        "  x = x.str.replace(r'\\'text\\'', '')\r\n",
        "  x = x.str.replace(r'\\'start\\'', '')\r\n",
        "  x = x.str.replace(r'\\'end\\'', '')\r\n",
        "  x = x.str.replace(r'[{}]', '')\r\n",
        "  x = x.str.replace(r'[<>]', '')\r\n",
        "  x = x.str.replace(r'[\\[\\]]', '')\r\n",
        "  x = x.str.replace(r'[\"]', '')\r\n",
        "  x = x.str.replace(r'[\\']', '')\r\n",
        "  x = x.str.replace(r'[:]', '')\r\n",
        "  x = x.str.replace(r'[()]', '')\r\n",
        "  x = x.str.replace(r'[?]', '')\r\n",
        "  return x"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSEosfCi3Eyk"
      },
      "source": [
        "dataset_df['body'] = removeCharacters(dataset_df['body'])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFXBu3TJ7oVv"
      },
      "source": [
        "Remove all the uppercase letters from title and body"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLj8oa3G0WOO"
      },
      "source": [
        "dataset_df['body'] = dataset_df['body'].apply(lambda x: x.lower())"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVNhJaYx72ie"
      },
      "source": [
        "**Dataframe after the preprocess**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPqJJQft1xP_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "7f054cad-d70a-45c1-c7f8-c22510c2a560"
      },
      "source": [
        "dataset_df.head(5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>236bd666a76213bc131969e1d5b66e410fc1cd45</td>\n",
              "      <td>MINI REVIEW Acute Phase Proteins in Marine Mam...</td>\n",
              "      <td>the mammalian immune system includes innate o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>14374db205f6934d9cba148624462000bc6ec7be</td>\n",
              "      <td>Antibody Treatment against Angiopoietin-Like 4...</td>\n",
              "      <td>importance despite extensive global efforts, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>af678e8cd31d74cdb2d690addc19d59dca331f2b</td>\n",
              "      <td>Quantifying the seasonal drivers of transmissi...</td>\n",
              "      <td>growing human population, urbanization and gl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>42b049c2b5b32c094dc8b10f967e43ac2169b890</td>\n",
              "      <td>Evaluation of the influenza-like illness surve...</td>\n",
              "      <td>the first evaluation of the tunisian influenz...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1664a9df618ca74e099245a2bd65f3172aeac284</td>\n",
              "      <td>nan</td>\n",
              "      <td>worldwide, lung cancer remains the most frequ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         id  ...                                               body\n",
              "0  236bd666a76213bc131969e1d5b66e410fc1cd45  ...   the mammalian immune system includes innate o...\n",
              "1  14374db205f6934d9cba148624462000bc6ec7be  ...   importance despite extensive global efforts, ...\n",
              "2  af678e8cd31d74cdb2d690addc19d59dca331f2b  ...   growing human population, urbanization and gl...\n",
              "3  42b049c2b5b32c094dc8b10f967e43ac2169b890  ...   the first evaluation of the tunisian influenz...\n",
              "4  1664a9df618ca74e099245a2bd65f3172aeac284  ...   worldwide, lung cancer remains the most frequ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez34v_Bockvu"
      },
      "source": [
        "%%capture\r\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii7T7CTmZG_z"
      },
      "source": [
        "%%capture\r\n",
        "!pip install sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIt22mNknCAN",
        "outputId": "40a67eac-7f60-4bec-8e5c-a0fc7c7e79bc"
      },
      "source": [
        "import torch\r\n",
        "# First checking if GPU is available\r\n",
        "train_on_gpu=torch.cuda.is_available()\r\n",
        "\r\n",
        "if(train_on_gpu):\r\n",
        "    print('Training on GPU.')\r\n",
        "    device = 'cuda'\r\n",
        "else:\r\n",
        "    print('No GPU available, training on CPU.')\r\n",
        "    device = 'cpu'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SkWJaK676jq"
      },
      "source": [
        "## Questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QSm-Z7E7535"
      },
      "source": [
        "questions =  [\r\n",
        "                  \"What are the coronoviruses?\",\r\n",
        "                  \"What was discovered in Wuhuan in December 2019?\",\r\n",
        "                  \"What is Coronovirus Disease 2019?\",\r\n",
        "                  \"What is COVID-19?\",\r\n",
        "                  \"What is caused by SARS-COV2?\",\r\n",
        "                  \"How is COVID-19 spread?\",\r\n",
        "                  \"Where was COVID-19 discovered?\",\r\n",
        "                  \"How does coronavirus spread?\",\r\n",
        "              ]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCnCjG_4FH6w"
      },
      "source": [
        "## Create the list of sentences\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCCiwa-vFWbI",
        "outputId": "3f90674a-392d-4de4-bb36-3716ea97ab5a"
      },
      "source": [
        "import nltk\r\n",
        "from nltk import tokenize\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "ListOfBodies = dataset_df['body'].apply(lambda x: tokenize.sent_tokenize(x))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUmlz-0HFlrQ"
      },
      "source": [
        "## Usefull functions for the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6IYr56uFl7e"
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "\r\n",
        "def most_similar(sentences, embeddings, query_embedding, k = 1):\r\n",
        "    X = np.stack(embeddings)\r\n",
        "    score_map = zip(sentences, cosine_similarity(X, query_embedding.reshape(1, -1)))\r\n",
        "    return sorted(score_map, key=lambda v: v[1], reverse=True)[:k]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY6BmUj--Aav"
      },
      "source": [
        "def ask_question(model,question,ListOfSentences,sentence_embeddings,dataset_df):\r\n",
        "  query_vec = model.encode([question])[0]\r\n",
        "  similar = most_similar(ListOfSentences,sentence_embeddings,query_vec)\r\n",
        "  row = dataset_df[dataset_df['body'].str.contains(similar[0][0])]\r\n",
        "  title = row['title'].tolist()[0]\r\n",
        "  text = similar[0][0]\r\n",
        "\r\n",
        "  return title,text"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNy5I8Hp-CLD"
      },
      "source": [
        "from termcolor import colored\r\n",
        "\r\n",
        "def print_answer(question,title,text):\r\n",
        "\r\n",
        "  print(colored(\"Question : \",attrs=['bold']),question)\r\n",
        "  print(\"\\n\")\r\n",
        "  print(colored(\"Title : \",attrs=['bold']),title)\r\n",
        "  print(\"\\n\")\r\n",
        "  print(colored(\"Text : \",attrs=['bold']),text)\r\n",
        "  print(\"\\n\")\r\n",
        "  # text = row['body'].apply(lambda x: tokenize.sent_tokenize(x))\r\n",
        "  # num = 0\r\n",
        "  # for t in text:\r\n",
        "  #   for sentence in t:\r\n",
        "  #     if sentence == l1:\r\n",
        "  #         print(t[num-1], end=\" \")\r\n",
        "  #         print(\"\\n\")\r\n",
        "  #         print(colored(sentence,'grey','on_yellow'), end=\" \")\r\n",
        "  #         print(\"\\n\")\r\n",
        "  #         print(t[num+1], end=\" \")\r\n",
        "  #         num+=1\r\n",
        "  # print(\"\\n\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GylXNd8qpxt"
      },
      "source": [
        "## Step 2.a - First sentence embedding approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X67jFz8cIgOK"
      },
      "source": [
        "### SBERT Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvT01qEtIs9k"
      },
      "source": [
        "Uses Sentence-BERT (SBERT), a modification of the BERT network using siamese and triplet networks that is able to derive semantically meaningful sentence embeddings. This allows more efficient semantic search, which is utilized in the following application.\r\n",
        "\r\n",
        "The siamese network architecture enables that fixed-sized vectors for input sentences can be derived. Using a similarity measure like cosine similarity or Manhatten / Euclidean distance, semantically similar sentences can be found. Cosine similarity is used in this work.\r\n",
        "\r\n",
        "SBERT fine tuned on NLI data which creates SOTA sentence embeddings, as reported in the [SBERT paper](https://arxiv.org/pdf/1908.10084.pdf).\r\n",
        "\r\n",
        "SBERT Framework example\r\n",
        "\r\n",
        "<img src=\"https://combine.se/wp-content/uploads/2019/09/3.png\" alt=\"Cord-19\" width=\"300\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSqzKlSxFITl"
      },
      "source": [
        "ListOfSentences = []\r\n",
        "numOfArticles = 0\r\n",
        "for text in ListOfBodies:\r\n",
        "  ListOfSentences += text\r\n",
        "  numOfArticles+=1\r\n",
        "  if(numOfArticles == 4000):\r\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb_H5FDgT5oW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61933707-9641-4418-f002-3fc6490b6f7f"
      },
      "source": [
        "import torch\r\n",
        "from sentence_transformers import SentenceTransformer\r\n",
        "\r\n",
        "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens',device=device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 405M/405M [00:14<00:00, 27.7MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nTaYmUq9HrT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "268f2cb6-77ec-4506-ee1d-1c1101ef2015"
      },
      "source": [
        "import time\r\n",
        "\r\n",
        "start_time = time.time()\r\n",
        "\r\n",
        "sentence_embeddings = sbert_model.encode(ListOfSentences,device=device)\r\n",
        "\r\n",
        "for question in questions: \r\n",
        "  title,text = ask_question(sbert_model,question,ListOfSentences,sentence_embeddings,dataset_df)\r\n",
        "  print(\"-------------------------------------------------------------------------\\n\")\r\n",
        "  print_answer(question, title, text)\r\n",
        "\r\n",
        "print(\"-------------------------------------------------------------------------\\n\")\r\n",
        "print(\"Time: %s seconds\" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mQuestion : \u001b[0m What are the coronoviruses?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m A viral metagenomic survey identifies known and novel mammalian viruses in bats from Saudi Arabia\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m c parechoviruses.\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mQuestion : \u001b[0m What was discovered in Wuhuan in December 2019?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m Old World camels in a modern world -a balancing act between conservation and genetic improvement\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m the photoperiod nagy & juhasz 2019 .\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mQuestion : \u001b[0m What is Coronovirus Disease 2019?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m Estimated effectiveness of symptom and risk screening to prevent the spread of COVID-19\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m hcov-19 has been proposed as an alternate name for the virus; jiang et al., 2020 .\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mQuestion : \u001b[0m What is COVID-19?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m Alignment-free method for DNA sequence clustering using Fuzzy integral similarity ROC_supplementary material Explanation\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m figure 19  our method.\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mQuestion : \u001b[0m What is caused by SARS-COV2?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m Nosocomial Transmission of Emerging Viruses via Aerosol-Generating Medical Procedures\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m the former was used to assess the risk of agmps and sars-cov transmission 11 .\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mQuestion : \u001b[0m How is COVID-19 spread?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m Alignment-free method for DNA sequence clustering using Fuzzy integral similarity ROC_supplementary material Explanation\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m figure 19  our method.\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mQuestion : \u001b[0m Where was COVID-19 discovered?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m Nucleocapsid protein-dependent assembly of the RNA packaging signal of Middle East respiratory syndrome coronavirus\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m plasmid pegfp-n1-ps580 has been previously described 19 .\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mQuestion : \u001b[0m How does coronavirus spread?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m Estimation of basic reproduction number of the Middle East respiratory syndrome coronavirus (MERS-CoV) during the outbreak in South Korea, 2015\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m  the middle east respiratory syndrome mers is caused by a coronavirus cov, the mers-cov.\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\n",
            "Time: 2970.1945104599 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVrg1Ba2q5p0"
      },
      "source": [
        "## Step 2.b - Second sentence embedding approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edptFiB3USfe"
      },
      "source": [
        "### InferSent Model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ID0sNizUtI8"
      },
      "source": [
        "InferSent is a sentence embeddings method that provides semantic representations for English sentences. It is trained on natural language inference data and generalizes well to many different tasks. Just like SentenceBERT, we take a pair of sentences and encode them to generate the actual sentence embeddings. Then, extract the relations between these embeddings using:\r\n",
        "\r\n",
        "* concatenation\r\n",
        "* element-wise product\r\n",
        "* absolute element-wise difference.\r\n",
        "\r\n",
        "<img src=\"https://miro.medium.com/max/972/1*efWq1UrOcGy2E-34OxsBHQ.png\" alt=\"InferSent\" width=\"300\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VyWhhZAEW0j",
        "outputId": "6116a80f-4627-43cb-f72d-d64f8f34f85c"
      },
      "source": [
        "! mkdir encoder\r\n",
        "! curl -Lo encoder/infersent2.pkl https://dl.fbaipublicfiles.com/infersent/infersent2.pkl\r\n",
        "  \r\n",
        "! mkdir GloVe\r\n",
        "! curl -Lo GloVe/glove.840B.300d.zip http://nlp.stanford.edu/data/glove.840B.300d.zip\r\n",
        "! unzip GloVe/glove.840B.300d.zip -d GloVe/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘encoder’: File exists\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  146M  100  146M    0     0  1024k      0  0:02:26  0:02:26 --:--:--  726k\n",
            "mkdir: cannot create directory ‘GloVe’: File exists\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0   315    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0   352    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 2075M  100 2075M    0     0  2046k      0  0:17:18  0:17:18 --:--:-- 2683k\n",
            "Archive:  GloVe/glove.840B.300d.zip\n",
            "replace GloVe/glove.840B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLyEMksVr7V_"
      },
      "source": [
        "drive.mount('/content/drive',force_remount=True)\r\n",
        "sys.path.append('/content/drive/My Drive/')\r\n",
        "!cp -r \"/content/drive/My Drive/models.py\" '/content/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdD1_dKhDnpU"
      },
      "source": [
        "ListOfSentences = []\r\n",
        "numOfArticles = 0\r\n",
        "for text in ListOfBodies:\r\n",
        "  ListOfSentences += text\r\n",
        "  numOfArticles+=1\r\n",
        "  if(numOfArticles == 100):\r\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WENjmLyhEa7s"
      },
      "source": [
        "import models\r\n",
        "from models import InferSent\r\n",
        "import torch \r\n",
        "\r\n",
        "V = 2\r\n",
        "MODEL_PATH = 'encoder/infersent%s.pkl' % V\r\n",
        "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\r\n",
        "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\r\n",
        "model = InferSent(params_model)\r\n",
        "model.load_state_dict(torch.load(MODEL_PATH))\r\n",
        "\r\n",
        "W2V_PATH = '/content/GloVe/glove.840B.300d.txt'\r\n",
        "model.set_w2v_path(W2V_PATH)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5puzY_qm5tGI"
      },
      "source": [
        "if(train_on_gpu):\r\n",
        "  model = model.cuda()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMQvkMZUtxcl"
      },
      "source": [
        "import time\r\n",
        "\r\n",
        "start_time = time.time()\r\n",
        "model.build_vocab(ListOfSentences, tokenize=True)\r\n",
        "\r\n",
        "InferSent_embeddings = []\r\n",
        "\r\n",
        "for sentence in ListOfSentences:\r\n",
        " InferSent_embeddings.append(model.encode(sentence)[0])\r\n",
        "\r\n",
        "for question in questions: \r\n",
        "  title,text = ask_question(model,question,ListOfSentences,InferSent_embeddings,dataset_df)\r\n",
        "  print(\"-------------------------------------------------------------------------\\n\")\r\n",
        "  print_answer(question, title, text)\r\n",
        "  \r\n",
        "print(\"-------------------------------------------------------------------------\\n\")\r\n",
        "print(\"Time: %s seconds \" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOoUb4l5nAoz"
      },
      "source": [
        "## Step 2.b - Third sentence embedding approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q54n_kwJm0ji"
      },
      "source": [
        "### Doc2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiIRdCLInc-W"
      },
      "source": [
        "Doc2vec is an unsupervised algorithm to generate vectors for sentence/paragraphs/documents. The algorithm is an adaptation of word2vec which can generate vectors for words. The vectors generated can be used for tasks like finding similarity between sentences/paragraphs/documents.\r\n",
        "\r\n",
        "<img src=\"https://miro.medium.com/max/972/0*x-gtU4UlO8FAsRvL.\" alt=\"InferSent\" width=\"400\"/>\r\n",
        "\r\n",
        "Εvery paragraph is mapped to a unique vector, represented by a column in matrix D and every word is also mapped to a unique vector, represented by a column in matrix W. The paragraph vector and word vectors are averaged or concatenated to predict the next word in a context. (This is for the above figure)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPVVpF_QS7h5"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "def Doc2Vec_ask_question(model, question, ListOfSentences,dataset_df):\r\n",
        "  test_doc = word_tokenize(question.lower())\r\n",
        "  test_doc_vector = model.infer_vector(test_doc)\r\n",
        "  similar = model.docvecs.most_similar(positive = [test_doc_vector])\r\n",
        "  text = ListOfSentences[similar[0][0]]\r\n",
        "  row = dataset_df[dataset_df['body'].str.contains(text)]\r\n",
        "  title = row['title'].tolist()\r\n",
        "\r\n",
        "  if not title:\r\n",
        "    title = row['id'].tolist()[0]   \r\n",
        "  else:\r\n",
        "    title = row['title'].tolist()[0]\r\n",
        "\r\n",
        "  return title,text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XwfadW9muSK"
      },
      "source": [
        "ListOfSentences = []\r\n",
        "numOfArticles = 0\r\n",
        "for text in ListOfBodies:\r\n",
        "  ListOfSentences += text\r\n",
        "  numOfArticles+=1\r\n",
        "  if(numOfArticles == 8000):\r\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in-_Rg82ncb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d963bf4d-5273-4a9f-8243-9c0a9f771007"
      },
      "source": [
        "import time\r\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\r\n",
        "\r\n",
        "start_time = time.time()\r\n",
        "\r\n",
        "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(ListOfSentences)]\r\n",
        "\r\n",
        "model = Doc2Vec(tagged_data, vector_size = 20, window = 2, min_count = 1)\r\n",
        "\r\n",
        "for question in questions: \r\n",
        "  title,text = Doc2Vec_ask_question(model,question,ListOfSentences,dataset_df)\r\n",
        "  print(\"-------------------------------------------------------------------------\\n\")\r\n",
        "  print_answer(question,title,text)\r\n",
        "\r\n",
        "\r\n",
        "print(\"-------------------------------------------------------------------------\\n\")\r\n",
        "print(\"Time: %s seconds \" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mQuestion : \u001b[0m What are the coronoviruses?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m Noroviruses subvert the core stress granule component G3BP1 to promote viral VPg-dependent translation\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m bv2 cells were maintained in dmem supplemented with 10% fcs biosera, 2 mm l-glutamine, 0.075% sodium bicarbonate gibco and the antibiotics penicillin and streptomycin.\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mQuestion : \u001b[0m What was discovered in Wuhuan in December 2019?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m Acute systemic DNA damage in youth does not impair immune defense with aging\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m overall, our results highlight remarkable resilience of the immune system to withstand extensive dna damage and continue to competently operate for life., cite_spans  556,  881,  hayashi, t. heather e. lynch, susan geyer, kengo yoshida, keiko furudoi, keiko sasaki, yukari morishita, hiroko nagamura, mayumi maki, yiqun hu, ikue hayashi, seishi kyoizumi, yoichiro kusunoki, waka ohishi, saeko fujiwara, ivo shterev, janko nikolich-zugich donna murasko, gregory d. sempowski, kei nakachi, in preparation, ref_id none, ref_spans , section discussion,  mice adult  6 month male c57bl/six mice were acquired from jackson laboratories and held under specific pathogen-free conditions in the animal facility at the university of arizona ua for life.\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mQuestion : \u001b[0m What is Coronovirus Disease 2019?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m Analysis of long non-coding RNAs in neonatal piglets at different stages of porcine deltacoronavirus infection\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m decreased expression of mir-133b is associated with poor survival and increased metastasis in colorectal cancer 30 .\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mQuestion : \u001b[0m What is COVID-19?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m In Situ Tagged nsp15 Reveals Interactions with Coronavirus Replication/Transcription Complex- Associated Proteins\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m we found that nsp12 coprecipitated with nsp8 in both ra59 nsp15-ha -and ra59 wt -infected cells, in agreement with previous results fig.\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mQuestion : \u001b[0m What is caused by SARS-COV2?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m A Diallel of the Mouse Collaborative Cross Founders Reveals Strong Strain-Specific Maternal Effects on Litter Size\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m 2014, ref_id bibref15, ref_spans , section diallel analysis of litter size,  to provide directional parent-of-origin effects for maternal and paternal strain we defined, by reparameterization of the additive and parental sex effect, the following two additional types of effect, cite_spans , ref_spans , section diallel analysis of litter size,  for example, dam b6 ¼ a b6 þ m b6 is the b6-specific dam maternal strain effect and sire b6 ¼ a b6 2 m b6 is the b6-specific sire paternal strain effect.\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mQuestion : \u001b[0m How is COVID-19 spread?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m Supplementary Figures\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m the reaction mixture was concentrated and 30 ml of etoac was added.\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mQuestion : \u001b[0m Where was COVID-19 discovered?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m microorganisms Disparate Entry of Adenoviruses Dictates Differential Innate Immune Responses on the Ocular Surface\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m ekc, which is most commonly caused by hadv-d8, -37, -53, -54, -56, and -64, is a severe, hyperacute, and particularly contagious infection 21, 22 .\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mQuestion : \u001b[0m How does coronavirus spread?\n",
            "\n",
            "\n",
            "\u001b[1mTitle : \u001b[0m Detection of infectious bronchitis virus 793B, avian metapneumovirus, Mycoplasma gallisepticum and Mycoplasma synoviae in poultry in Ethiopia\n",
            "\n",
            "\n",
            "\u001b[1mText : \u001b[0m therefore, it has to continually receive and maintain several small groups of poultry of different ages, and even with quarantine precautions, it is unlikely that eradication will be feasible, as an all-in all-out system will never be practical with the current housing facilities., cite_spans  399,  430,  kleven and ferguson-noel 2008, ref_id bibref24,  504,  526,  mohammed et al.\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "\n",
            "Time: 902.1361558437347 seconds \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWSAT3IxrBZ4"
      },
      "source": [
        "## Step 3 - Comparison of three models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu9jcIFM-eMi"
      },
      "source": [
        "## References\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "*   https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/\r\n",
        "*   https://github.com/facebookresearch/InferSent\r\n",
        "*  https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5\r\n",
        "* https://kanoki.org/2019/03/07/sentence-similarity-in-python-using-doc2vec/\r\n",
        "\r\n"
      ]
    }
  ]
}